{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-07T02:32:54.776247Z",
     "start_time": "2024-10-07T02:32:48.185169Z"
    }
   },
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "print(torch.cuda.is_available())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T08:58:02.876791Z",
     "start_time": "2024-10-06T08:58:01.749380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = \"./deep_learning/datasets/CharAll_na_rm_huge_train_variableall4_sentiment_full_new.npz\"\n",
    "split_lists = np.load('./deep_learning/sampling_folds/random_sampling_folds.npy', allow_pickle = True)\n",
    "dataset = np.load(data_path)\n",
    "data = dataset['data']"
   ],
   "id": "802a4b7686174c67",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T03:48:05.021030Z",
     "start_time": "2024-10-07T03:48:05.006651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subset2col = {\n",
    "\t'flow+fund_mom+sentiment': list(range(56, 60))+[47],\n",
    "\t'fund_ex_mom_flow': [59]+ [x for x in range(46, 58) if x not in (list(range(54, 58))+[47])],\n",
    "\t'stock': range(46),\n",
    "\t'fund': range(46, 59),\n",
    "\t'fund+sentiment': range(46, 60),\n",
    "\t'stock+fund': range(59),\n",
    "\t'F_r12_2+sentiment': [58, 59],\n",
    "\t'stock+sentiment': [59]+list(range(0, 46)),\n",
    "\t'stock+fund+sentiment': range(60),\n",
    "\t'F_r12_2+flow+sentiment': [47, 58, 59]\n",
    "}\n",
    "\n",
    "def squeeze_data(data, UNK = -99.99):\n",
    "\tT, N, M = data.shape\n",
    "\tlists_considered = []    \n",
    "\treturns = data[:,:,0]    \n",
    "\tfor i in range(N):      \n",
    "\t\treturns_i = returns[:,i]             \n",
    "\t\tif np.sum(returns_i!=UNK)>0:        \n",
    "\t\t\tlists_considered.append(i)         \n",
    "\treturn data[:, lists_considered, :], lists_considered\n",
    "\n",
    "class FirmChar:\n",
    "\tdef __init__(self):\n",
    "\t\tself._category = ['Fund mom','Fund char', 'Fund Family', 'Sentiment']\n",
    "\t\tself._category2variables = {\n",
    "\t\t\t'Fund mom': ['F_ST_Rev', 'F_r2_1', 'F_r12_2'],\n",
    "\t\t\t'Fund char': ['ages', 'flow', 'exp_ratio', 'tna', 'turnover'],\n",
    "\t\t\t'Fund Family': ['Family_TNA', 'fund_no', 'Family_r12_2', 'Family_flow', 'Family_age'], \n",
    "\t\t\t'Sentiment': ['sentiment', 'RecCFNAI', 'sentiment_lsq', 'sentiment_lad', 'CFNAI_orth', 'leading'], \n",
    "\t\t}\n",
    "\t\tself._variable2category = {}\n",
    "\t\tfor category in self._category:\n",
    "\t\t\tfor var in self._category2variables[category]:\n",
    "\t\t\t\tself._variable2category[var] = category\n",
    "\t\tself._category2color = {\n",
    "\t\t\t'Fund mom': 'blue',\n",
    "\t\t\t'Fund char': 'plum',\n",
    "\t\t\t'Fund Family':'lime',\n",
    "\t\t\t'Sentiment':'darkgreen'\n",
    "\t\t}\n",
    "\t\tself._color2category = {value:key for key, value in self._category2color.items()}\n",
    "\n",
    "\tdef getColorLabelMap(self):       \n",
    "\t\treturn {var: self._category2color[self._variable2category[var]] for var in self._variable2category}\n",
    "\n",
    "\n",
    "def get_data(data_path, split_list, subset):\n",
    "\tdataset = np.load(data_path)\n",
    "\tdata = dataset['data']\n",
    "\tcolumn_considered = [0]+[x+1 for x in subset2col[subset]]        \n",
    "\tdata = data[:,:,column_considered]       \n",
    "\tdata, list_considered = squeeze_data(data[split_list])\n",
    "\treturn data, list_considered\n",
    "\n",
    "def get_tensors(data, UNK = -99.99):\n",
    "\tret = torch.tensor(data[:,:,0])\n",
    "\tindividualFeature = torch.tensor(data[:,:,1:] )    \n",
    "\tmacroFeature = torch.empty((data.shape[0], 0))\n",
    "\tmask = (ret != UNK)\n",
    "\t\n",
    "\tinput_macro_tile = macroFeature.unsqueeze(1).repeat(1, ret.shape[1], 1)\n",
    "\tinput_macro_masked = input_macro_tile[mask]\n",
    "\tinput_masked = individualFeature[mask]\n",
    "\tinput_concat = torch.concat([input_masked, input_macro_masked], dim=1)\n",
    "\treturn_masked = ret[mask]\n",
    "\t\n",
    "\treturn input_concat, return_masked, mask\n",
    "\n",
    "def get_dataset(data_path, split_list, subset):\n",
    "\tdatasets = []\n",
    "\tmasks = []\n",
    "\tfor split in split_list:\n",
    "\t\tdata, list_considered = get_data(data_path, split, subset)\n",
    "\t\tinput_concat, return_masked, mask = get_tensors(data)\n",
    "\t\tdatasets.append(torch.utils.data.TensorDataset(input_concat, return_masked))\n",
    "\t\tmasks.append(mask)\n",
    "\treturn datasets, masks\n",
    "\n",
    "def get_dataloader(datasets, batch_size, num_workers=4, shuffle=False):\n",
    "\tdataloaders = []\n",
    "\tfor dataset in datasets:\n",
    "\t\tdataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
    "\t\tdataloaders.append(dataloader)\n",
    "\treturn dataloaders\n",
    "\n",
    "def get_crossval_dataloaders(data_path, split_lists, subset, batch_size, num_workers=4, shuffle=False):\n",
    "\tcrossval_loaders = []\n",
    "\tmasks = []\n",
    "\tfor split_list in split_lists:\t\n",
    "\t\tdatasets, mask = get_dataset(data_path, split_list, subset)\n",
    "\t\tdataloaders = get_dataloader(datasets, batch_size, num_workers, shuffle)\n",
    "\t\tdict = {'datasets': datasets,\n",
    "\t\t\t\t'dataloaders': dataloaders}\n",
    "\t\tmasks.append(mask)\n",
    "\t\tcrossval_loaders.append(dict)\n",
    "\treturn crossval_loaders, masks\n",
    "\n",
    "def evaluate_sharpe(r_pred, r_masked, mask):\n",
    "\tportfolio = construct_long_short_portfolio(r_pred, r_masked, mask, low=0.1, high=0.1) # equally weighted\n",
    "\treturn sharpe(portfolio)\n",
    "\n",
    "def sharpe(r):\n",
    "\treturn torch.mean(r / r.std())\n",
    "\n",
    "def construct_decile_portfolios(w, R, mask, value=None, decile=10):\n",
    "    N_i = torch.sum(mask.int(), dim=1)\n",
    "    \n",
    "    # Splitting the tensors based on cumulative sums\n",
    "    w_split = torch.split(w, N_i.tolist())\n",
    "    R_split = torch.split(R, N_i.tolist())\n",
    "\n",
    "    value_weighted = False\n",
    "    if value is not None:\n",
    "        value_weighted = True\n",
    "        value = value[mask]\n",
    "        value_split = torch.split(value, N_i.tolist())\n",
    "\n",
    "    portfolio_returns = []\n",
    "\n",
    "    for j in range(mask.size(0)):  # Iterate over rows\n",
    "        R_j = R_split[j]\n",
    "        w_j = w_split[j]\n",
    "        if value_weighted:\n",
    "            value_j = value_split[j]\n",
    "            R_w_j = [(R_j[k].item(), w_j[k].item(), value_j[k].item()) for k in range(N_i[j])]\n",
    "        else:\n",
    "            R_w_j = [(R_j[k].item(), w_j[k].item(), 1.0) for k in range(N_i[j])]\n",
    "\n",
    "        # Sort by weights\n",
    "        R_w_j_sorted = sorted(R_w_j, key=lambda t: t[1])\n",
    "\n",
    "        n_decile = N_i[j] // decile\n",
    "        R_decile = []\n",
    "        for i in range(decile):\n",
    "            R_decile_i = 0.0\n",
    "            value_sum_i = 0.0\n",
    "            for k in range(n_decile):\n",
    "                R_decile_i += R_w_j_sorted[i * n_decile + k][0] * R_w_j_sorted[i * n_decile + k][2]\n",
    "                value_sum_i += R_w_j_sorted[i * n_decile + k][2]\n",
    "            R_decile.append(R_decile_i / value_sum_i)\n",
    "        portfolio_returns.append(R_decile)\n",
    "\n",
    "    return torch.tensor(portfolio_returns)\n",
    "\n",
    "\n",
    "def construct_long_short_portfolio(w, R, mask, value=None, low=0.1, high=0.1, normalize=True):\n",
    "\tN_i = torch.sum(mask.int(), dim=1)\n",
    "\n",
    "\t# Splitting the tensors based on cumulative sums\n",
    "\tw_split = torch.split(w, N_i.tolist())\n",
    "\tR_split = torch.split(R, N_i.tolist())\n",
    "\t\n",
    "\tvalue_weighted = False\n",
    "\tif value is not None:\n",
    "\t\tvalue_weighted = True\n",
    "\t\tvalue_split = torch.split(value[mask], N_i.tolist())\n",
    "\t\n",
    "\tportfolio_returns = []\n",
    "\t\n",
    "\tfor j in range(mask.size(0)):  # Iterate over rows\n",
    "\t\tR_j = R_split[j]\n",
    "\t\tw_j = w_split[j]\n",
    "\t\tif value_weighted:\n",
    "\t\t\tvalue_j = value_split[j]\n",
    "\t\t\tR_w_j = [(R_j[k].item(), w_j[k].item(), value_j[k].item()) for k in range(N_i[j])]\n",
    "\t\telse:\n",
    "\t\t\tR_w_j = [(R_j[k].item(), w_j[k].item(), 1.) for k in range(N_i[j])]\n",
    "\t\n",
    "\t\t# Sort by weights\n",
    "\t\tR_w_j_sorted = sorted(R_w_j, key=lambda t: t[1])\n",
    "\t\n",
    "\t\t# Calculate low and high portfolio returns\n",
    "\t\tn_low = int(low * N_i[j])\n",
    "\t\tn_high = int(high * N_i[j])\n",
    "\t\n",
    "\t\tportfolio_return_high = 0.0\n",
    "\t\tvalue_sum_high = 0.0\n",
    "\t\tif n_high > 0:\n",
    "\t\t\tfor k in range(n_high):\n",
    "\t\t\t\tportfolio_return_high += R_w_j_sorted[-k - 1][0] * R_w_j_sorted[-k - 1][2]\n",
    "\t\t\t\tvalue_sum_high += R_w_j_sorted[-k - 1][2]\n",
    "\t\t\tif normalize:\n",
    "\t\t\t\tportfolio_return_high /= value_sum_high\n",
    "\t\n",
    "\t\tportfolio_return_low = 0.0\n",
    "\t\tvalue_sum_low = 0.0\n",
    "\t\tif n_low > 0:\n",
    "\t\t\tfor k in range(n_low):\n",
    "\t\t\t\tportfolio_return_low += R_w_j_sorted[k][0] * R_w_j_sorted[k][2]\n",
    "\t\t\t\tvalue_sum_low += R_w_j_sorted[k][2]\n",
    "\t\t\tif normalize:\n",
    "\t\t\t\tportfolio_return_low /= value_sum_low\n",
    "\t\n",
    "\t\tportfolio_returns.append(portfolio_return_high - portfolio_return_low)\n",
    "\t\t\n",
    "\treturn torch.tensor(portfolio_returns)"
   ],
   "id": "c67d5e32e1a3c6f5",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T04:15:42.421213Z",
     "start_time": "2024-10-07T04:15:42.416119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Deep_Network(torch.nn.Module):\n",
    "\t'''\n",
    "\tThe module class performs building network according to config\n",
    "    '''\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper(Deep_Network, self).__init__()\n",
    "\t\t# parses parameters of network from configuration\n",
    "\t\tself.dropout = config['dropout']\n",
    "\t\tself.num_layers = config['num_layers']\n",
    "\t\tself.hidden_dim = config['hidden_dim']\n",
    "\t\tself.input_dim = config['input_dim']\n",
    "\t\t\n",
    "        # builds network\n",
    "\t\tself.hidden_layers = torch.nn.ModuleList()\n",
    "\t\tfor i in range(self.num_layers):\n",
    "\t\t\tinput_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\n",
    "\t\t\tself.hidden_layers.append(torch.nn.Linear(input_dim, self.hidden_dim[i]))\n",
    "\t\t\n",
    "\t\tself.dropout_layer = torch.nn.Dropout(self.dropout)\n",
    "\t\tself.output_layer = torch.nn.Linear(self.hidden_dim[-1], 1)\n",
    "\t\n",
    "\tdef forward(self, X):\n",
    "\t\tfor layer in self.hidden_layers:\n",
    "\t\t\tX = layer(X)\n",
    "\t\t\tX = torch.nn.functional.relu(X)\n",
    "\t\t\tX = self.dropout_layer(X)\n",
    "\t\treturn self.output_layer(X).squeeze(-1)"
   ],
   "id": "293b1b433573e3df",
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T04:15:43.093627Z",
     "start_time": "2024-10-07T04:15:43.088636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = {\n",
    "\t'data_path': \"./deep_learning/datasets/CharAll_na_rm_huge_train_variableall4_sentiment_full_new.npz\",\n",
    "\t'split_lists_path': './deep_learning/sampling_folds/random_sampling_folds.npy',\n",
    "\t'subset': 'fund+sentiment',\n",
    "\t'num_layers': 1,\n",
    "\t'hidden_dim': [2**6],\n",
    "\t'dropout': 0.95,\n",
    "\t'learning_rate': 0.001,\n",
    "\t'epochs': 512,\n",
    "\t'weighted_loss': False,\n",
    "\t'reg_l1': 0.0,\n",
    "\t'reg_l2': 0.01,\n",
    "\t'batch_size': 2048,\n",
    "\t'criteria': 'Factor_sharpe',\n",
    "\t'random_seed': 15,\n",
    "\t'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "\t'num_workers': 4\n",
    "}\n",
    "\n",
    "config['split_lists'] = np.load(config['split_lists_path'], allow_pickle = True)\n",
    "config['input_dim'] = len(subset2col[config['subset']])"
   ],
   "id": "fbf2dd22cac95c33",
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T04:15:52.799497Z",
     "start_time": "2024-10-07T04:15:44.747806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "crossval_loaders, masks = get_crossval_dataloaders(config['data_path'], config['split_lists'], 'fund+sentiment', batch_size=config['batch_size'])\n",
    "\n",
    "for i in range(len(crossval_loaders)):\n",
    "\tprint('Cross-validation fold {}'.format(i+1))\n",
    "\tfor j in crossval_loaders[i]['datasets']:\n",
    "\t\tprint(len(j))\n",
    "\tprint('')"
   ],
   "id": "33bf3fb7b7566b51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation fold 1\n",
      "208161\n",
      "68181\n",
      "130816\n",
      "\n",
      "Cross-validation fold 2\n",
      "198258\n",
      "68181\n",
      "140719\n",
      "\n",
      "Cross-validation fold 3\n",
      "208191\n",
      "63344\n",
      "135623\n",
      "\n"
     ]
    }
   ],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T04:18:59.769761Z",
     "start_time": "2024-10-07T04:18:59.765674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fold = 0\n",
    "train, val, test = crossval_loaders[fold]['datasets']\n",
    "train_loader, val_loader, test_loader = crossval_loaders[fold]['dataloaders']\n",
    "train_mask, val_mask, test_mask = masks[fold]"
   ],
   "id": "469cb60fba4040d9",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T04:18:59.993065Z",
     "start_time": "2024-10-07T04:18:59.987022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Deep_Network(config).to(device=config['device'])\n",
    "print(model)"
   ],
   "id": "8e5ece52bc6fe185",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep_Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=14, out_features=64, bias=True)\n",
      "  )\n",
      "  (dropout_layer): Dropout(p=0.95, inplace=False)\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-07T04:19:03.018333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training function\n",
    "torch.manual_seed(config['random_seed'])\n",
    "torch.cuda.manual_seed(config['random_seed'])\n",
    "torch.cuda.manual_seed_all(config['random_seed'])\n",
    "np.random.seed(config['random_seed'])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['reg_l2'])\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "best_model_state = model.state_dict()\n",
    "best_val_loss = torch.tensor(float('inf'))\n",
    "best_val_sharpe = torch.tensor(-float('inf'))\n",
    "sharpe_train = []\n",
    "sharpe_val = []\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "\n",
    "time_start = time.time()\n",
    "for epoch in range(config['epochs']):\n",
    "\tepoch_loss = torch.tensor(0.0)\n",
    "\ty_train = []\n",
    "\ty_pred_train = []\n",
    "\tfor i, (X, y) in enumerate(train_loader):\n",
    "\t\tX, y = X.float().to(config['device']), y.float().to(config['device'])\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\ty_pred = model(X)\n",
    "\t\t\n",
    "\t\tloss = criterion(y_pred, y)\t\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\t\n",
    "\t\tepoch_loss += loss.detach().cpu().item()\n",
    "\t\t\n",
    "\t\ty_train.append(y.detach().cpu())\n",
    "\t\ty_pred_train.append(y_pred.detach().cpu())\n",
    "\t\t\n",
    "\tepoch_loss /= i + 1\n",
    "\ty_train = torch.cat(y_train, dim=0)\n",
    "\ty_pred_train = torch.cat(y_pred_train, dim=0)\n",
    "\t\n",
    "\tsharpe_train_value = evaluate_sharpe(y_pred_train, y_train, train_mask)\n",
    "\tepoch_sharpe = sharpe_train_value.detach().cpu().item()\n",
    "\tsharpe_train.append(epoch_sharpe)\n",
    "\tloss_train.append(epoch_loss)\n",
    "\t\n",
    "\t# Validation\n",
    "\twith torch.no_grad():\n",
    "\t\tepoch_val_loss = torch.tensor(0.0)\n",
    "\t\ty_val = []\n",
    "\t\ty_pred_val = []\n",
    "\t\tfor i, (X, y) in enumerate(val_loader):\n",
    "\t\t\tX, y = X.float().to(config['device']), y.float().to(config['device'])\n",
    "\t\t\ty_pred = model(X)\n",
    "\t\t\tloss = criterion(y_pred, y)\n",
    "\t\t\tepoch_val_loss += loss.detach().cpu().item()\n",
    "\t\t\t\n",
    "\t\t\ty_val.append(y.detach().cpu())\n",
    "\t\t\ty_pred_val.append(y_pred.detach().cpu())\n",
    "\t\t\t\n",
    "\tepoch_val_loss /= i + 1\n",
    "\ty_val = torch.cat(y_val, dim=0)\n",
    "\ty_pred_val = torch.cat(y_pred_val, dim=0)\n",
    "\t\n",
    "\tsharpe_val_value = evaluate_sharpe(y_pred_val, y_val, val_mask)\n",
    "\tepoch_val_sharpe = sharpe_val_value.detach().cpu().item()\n",
    "\tsharpe_val.append(epoch_val_sharpe)\n",
    "\tloss_val.append(epoch_val_loss)\n",
    "\t\n",
    "\tif epoch <= 50 or epoch % 10 == 0:\n",
    "\t\tprint('Epoch {} - Training Loss: {:.8f}, Val Loss: {:.8f}, Train Sharpe: {:.8f}, Validation Sharpe: {:.8f}'.format(epoch+1, epoch_loss, epoch_val_loss, epoch_sharpe, epoch_val_sharpe))\n",
    "\t\n",
    "\tif config['criteria'] == 'Factor_sharpe':\n",
    "\t\tif epoch_val_sharpe > best_val_sharpe:\n",
    "\t\t\tbest_val_sharpe = epoch_val_sharpe\n",
    "\t\t\tbest_model_state = model.state_dict()\n",
    "\t\t\tprint(\"Best model updated\")\n",
    "\t\n",
    "\telif epoch_val_loss < best_val_loss:\n",
    "\t\tbest_val_loss = epoch_val_loss\n",
    "\t\tbest_model_state = model.state_dict()\n",
    "\t\tprint(\"Best model updated\")\n",
    "\n",
    "exp_path = './Experiments/'\n",
    "exp_subset_path = os.path.join(exp_path, config['subset'])\n",
    "if not os.path.exists(exp_subset_path):\n",
    "\tos.makedirs(exp_subset_path)\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "time_stamp = datetime.datetime.today().strftime('%Y%m%d_%H%M')\n",
    "model_save_path = os.path.join(exp_subset_path, 'model' + str(fold+1) + '_' + config['subset'] + '_' + time_stamp + '.pth')\n",
    "torch.save(model, model_save_path)\n",
    "\n",
    "duration = time.time() - time_start\n",
    "print('Training completed in {:.0f}m {:.0f}s'.format(duration // 60, duration % 60))"
   ],
   "id": "e847033d7d69c0aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.06101711, Val Loss: 0.01324732, Train Sharpe: -0.10112385, Validation Sharpe: 0.05833856\n",
      "Best model updated\n",
      "Epoch 2 - Training Loss: 0.00481178, Val Loss: 0.00257534, Train Sharpe: 0.07646226, Validation Sharpe: 0.11010505\n",
      "Best model updated\n",
      "Epoch 3 - Training Loss: 0.00124002, Val Loss: 0.00100351, Train Sharpe: -0.03259716, Validation Sharpe: 0.10403872\n",
      "Epoch 4 - Training Loss: 0.00063398, Val Loss: 0.00061113, Train Sharpe: 0.03984211, Validation Sharpe: 0.03449043\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_train, label='Training Loss')\n",
    "plt.plot(loss_val, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(sharpe_train, label='Training Sharpe')\n",
    "plt.plot(sharpe_val, label='Validation Sharpe')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "c9f81fed21a6f306",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_one_epoch(model, train_loader, mask, optimizer, criterion, config):\n",
    "\tepoch_loss = torch.tensor(0.0)\n",
    "\ty_train = []\n",
    "\ty_pred_train = []\n",
    "\t\n",
    "\tfor i, (X, y) in enumerate(train_loader):\n",
    "\t\tX, y = X.float().to(config['device']), y.float().to(config['device'])\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\ty_pred = model(X)\n",
    "\t\t\n",
    "\t\tloss = criterion(y_pred, y)\t\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\t\n",
    "\t\tepoch_loss += loss.detach().cpu().item()\n",
    "\t\t\n",
    "\t\ty_train.append(y.detach().cpu())\n",
    "\t\ty_pred_train.append(y_pred.detach().cpu())\n",
    "\t\n",
    "\tepoch_loss /= i + 1\n",
    "\ty_train = torch.cat(y_train, dim=0)\n",
    "\ty_pred_train = torch.cat(y_pred_train, dim=0)\n",
    "\tepoch_sharpe = evaluate_sharpe(y_pred_train, y_train, mask).detach().cpu().item()\n",
    "\t\n",
    "\treturn epoch_loss, epoch_sharpe\n",
    "\n",
    "def validate(model, val_loader, mask, criterion, config):\n",
    "\t# Validation\n",
    "\twith torch.no_grad():\n",
    "\t\tepoch_val_loss = torch.tensor(0.0)\n",
    "\t\ty_val = []\n",
    "\t\ty_pred_val = []\n",
    "\t\tfor i, (X, y) in enumerate(val_loader):\n",
    "\t\t\tX, y = X.float().to(config['device']), y.float().to(config['device'])\n",
    "\t\t\ty_pred = model(X)\n",
    "\t\t\t\n",
    "\t\t\tloss = criterion(y_pred, y)\n",
    "\t\t\tepoch_val_loss += loss.detach().cpu().item()\n",
    "\t\t\ty_val.append(y.detach().cpu())\n",
    "\t\t\ty_pred_val.append(y_pred.detach().cpu())\n",
    "\t\t\t\n",
    "\tepoch_val_loss /= i + 1\n",
    "\ty_val = torch.cat(y_val, dim=0)\n",
    "\ty_pred_val = torch.cat(y_pred_val, dim=0)\n",
    "\tepoch_val_sharpe = evaluate_sharpe(y_pred_val, y_val, mask).detach().cpu().item()\n",
    "\t\n",
    "\treturn epoch_val_loss, epoch_val_sharpe\n",
    "\n",
    "def training(model, train_loader, val_loader, train_mask, val_mask, config, fold):\n",
    "\ttorch.manual_seed(config['random_seed'])\n",
    "\ttorch.cuda.manual_seed(config['random_seed'])\n",
    "\ttorch.cuda.manual_seed_all(config['random_seed'])\n",
    "\tnp.random.seed(config['random_seed'])\n",
    "\t\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['reg_l2'])\n",
    "\tcriterion = torch.nn.MSELoss()\n",
    "\tbest_model_state = model.state_dict()\n",
    "\tbest_val_loss = torch.tensor(float('inf'))\n",
    "\tbest_val_sharpe = torch.tensor(-float('inf'))\n",
    "\t\n",
    "\tsharpe_train = []\n",
    "\tsharpe_val = []\n",
    "\tloss_train = []\n",
    "\tloss_val = []\n",
    "\t\n",
    "\ttime_start = time.time()\n",
    "\tfor epoch in range(config['epochs']):\n",
    "\t\tepoch_loss, epoch_sharpe = train_one_epoch(model, train_loader, train_mask, optimizer, criterion, config)\n",
    "\t\tloss_train.append(epoch_loss)\n",
    "\t\tsharpe_train.append(epoch_sharpe)\n",
    "\t\t\n",
    "\t\t# Validation\n",
    "\t\tepoch_val_loss, epoch_val_sharpe = validate(model, val_loader, val_mask, criterion, config)\n",
    "\t\tloss_val.append(epoch_val_loss)\n",
    "\t\tsharpe_val.append(epoch_val_sharpe)\n",
    "\t\t\n",
    "\t\tif epoch <= 50 or epoch % 10 == 0:\n",
    "\t\t\tprint('Epoch {} - Training Loss: {:.8f}, Val Loss: {:.8f}, Train Sharpe: {:.8f}, Validation Sharpe: {:.8f}'.format(epoch+1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   epoch_loss,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   epoch_val_loss,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   epoch_sharpe,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   epoch_val_sharpe))\n",
    "\n",
    "\t\tif epoch_val_sharpe > best_val_sharpe and config['criteria'] == 'Factor_sharpe':\n",
    "\t\t\tbest_val_sharpe = epoch_val_sharpe\n",
    "\t\t\tbest_model_state = model.state_dict()\n",
    "\t\t\tprint(\"Best model updated\")\n",
    "\t\t\n",
    "\t\telif epoch_val_loss < best_val_loss:\n",
    "\t\t\tbest_val_loss = epoch_val_loss\n",
    "\t\t\tbest_model_state = model.state_dict()\n",
    "\t\t\tprint(\"Best model updated\")\n",
    "\t\n",
    "\texp_path = './Experiments/'\n",
    "\texp_subset_path = os.path.join(exp_path, config['subset'])\n",
    "\tif not os.path.exists(exp_subset_path):\n",
    "\t\tos.makedirs(exp_subset_path)\n",
    "\t\n",
    "\tmodel.load_state_dict(best_model_state)\n",
    "\ttime_stamp = datetime.datetime.today().strftime('%Y%m%d_%H%M')\n",
    "\tmodel_save_path = os.path.join(exp_subset_path, 'model' + str(fold+1) + '_' + config['subset'] + '_' + time_stamp + '.pth')\n",
    "\ttorch.save(model, model_save_path)\n",
    "\t\n",
    "\tduration = time.time() - time_start\n",
    "\tprint('Training completed in {:.0f}m {:.0f}s'.format(duration // 60, duration % 60))\n",
    "\treturn model, loss_train, loss_val, sharpe_train, sharpe_val"
   ],
   "id": "7ce1ed76389f0bea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "crossval_loaders = get_crossval_dataloaders(config['data_path'], config['split_lists'], config['subset'], batch_size=config['batch_size'])\n",
    "\n",
    "avg_loss = {'train': 0.0, 'val': 0.0, 'test': 0.0}\n",
    "avg_sharpe = {'train': 0.0, 'val': 0.0, 'test': 0.0}\n",
    "\n",
    "for fold in range(len(crossval_loaders)):\n",
    "\ttrain, val, test = crossval_loaders[fold]['datasets']\n",
    "\ttrain_loader, val_loader, test_loader = crossval_loaders[fold]['dataloaders']\n",
    "\tmodel = Deep_Network(config).to(device=config['device'])\n",
    "\tmodel, loss_train, loss_val, sharpe_train, sharpe_val = training(model, train_loader, val_loader, config, fold)\n",
    "\ttest_loss, test_sharpe = validate(model, test_loader, torch.nn.MSELoss(), config)\n",
    "\t\n",
    "\tavg_loss['train'] += loss_train[-1]\n",
    "\tavg_loss['val'] += loss_val[-1]\n",
    "\tavg_loss['test'] += test_loss\n",
    "\t\n",
    "\tavg_sharpe['train'] += sharpe_train[-1]\n",
    "\tavg_sharpe['val'] += sharpe_val[-1]\n",
    "\tavg_sharpe['test'] += test_sharpe\n",
    "\n",
    "avg_loss = {key: value / (fold+1) for key, value in avg_loss.items()}\n",
    "avg_sharpe = {key: value / (fold+1) for key, value in avg_sharpe.items()}"
   ],
   "id": "cd14d59df4673855"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_one_subset(config):\n",
    "\tprint('Running subset: {}'.format(config['subset']))\n",
    "\tcrossval_loaders = get_crossval_dataloaders(config['data_path'], config['split_lists'], config['subset'], batch_size=config['batch_size'])\n",
    "\n",
    "\tavg_loss = {'train': 0.0, 'val': 0.0, 'test': 0.0}\n",
    "\tavg_sharpe = {'train': 0.0, 'val': 0.0, 'test': 0.0}\n",
    "\t\n",
    "\tfor fold in range(len(crossval_loaders)):\n",
    "\t\tprint('Running fold no. {}'.format(fold+1))\n",
    "\t\ttrain_loader, val_loader, test_loader = crossval_loaders[fold]['dataloaders']\n",
    "\t\ttrain_mask, val_mask, test_mask = masks[fold]\n",
    "\t\tmodel = Deep_Network(config).to(device=config['device'])\n",
    "\t\tmodel, loss_train, loss_val, sharpe_train, sharpe_val = training(model, train_loader, val_loader, train_mask, val_mask, config, fold)\n",
    "\t\ttest_loss, test_sharpe = validate(model, test_loader, test_mask, torch.nn.MSELoss(), config)\n",
    "\t\t\n",
    "\t\tavg_loss['train'] += loss_train[-1]\n",
    "\t\tavg_loss['val'] += loss_val[-1]\n",
    "\t\tavg_loss['test'] += test_loss\n",
    "\t\t\n",
    "\t\tavg_sharpe['train'] += sharpe_train[-1]\n",
    "\t\tavg_sharpe['val'] += sharpe_val[-1]\n",
    "\t\tavg_sharpe['test'] += test_sharpe\n",
    "\t\n",
    "\tavg_loss = {key: value / (fold+1) for key, value in avg_loss.items()}\n",
    "\tavg_sharpe = {key: value / (fold+1) for key, value in avg_sharpe.items()}\n",
    "\treturn avg_loss, avg_sharpe\n",
    "\n",
    "def run_all_subsets(config):\n",
    "\tlosses = {}\n",
    "\tsharpes = {}\n",
    "\tfor subset in subset2col.keys():\n",
    "\t\tconfig['subset'] = subset\n",
    "\t\tconfig['input_dim'] = len(subset2col[config['subset']])\n",
    "\t\tlosses[subset], sharpes[subset] = run_one_subset(config)\n",
    "\treturn losses, sharpes"
   ],
   "id": "131c956f191287de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tlosses, sharpes = run_all_subsets(config)\n",
    "\tprint(losses)\n",
    "\tprint(sharpes)"
   ],
   "id": "978880446dddc07b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
