{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-11T10:21:10.116972Z",
     "start_time": "2024-10-11T10:21:08.507760Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from models import DeepNetwork, DeepEnsemble\n",
    "from utils import *\n",
    "from train import *\n",
    "\n",
    "print(torch.cuda.is_available())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T10:21:10.340152Z",
     "start_time": "2024-10-11T10:21:10.327955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subset2col = {\n",
    "        'flow+fund_mom+sentiment': list(range(56, 60)) + [47],\n",
    "        'fund_ex_mom_flow': [59] + [x for x in range(46, 58) if x not in (list(range(54, 58)) + [47])],\n",
    "        'stock': range(46),\n",
    "        'fund': range(46, 59),\n",
    "        'fund+sentiment': range(46, 60),\n",
    "        'stock+fund': range(59),\n",
    "        'F_r12_2+sentiment': [58, 59],\n",
    "        'stock+sentiment': [59] + list(range(0, 46)),\n",
    "        'stock+fund+sentiment': range(60),\n",
    "        'F_r12_2+flow+sentiment': [47, 58, 59]\n",
    "    }"
   ],
   "id": "802a4b7686174c67",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T10:21:11.112666Z",
     "start_time": "2024-10-11T10:21:11.097220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = {\n",
    "\t'data_path': \"./deep_learning/datasets/CharAll_na_rm_huge_train_variableall4_sentiment_full_new.npz\",\n",
    "\t'split_lists_path': './deep_learning/sampling_folds/random_sampling_folds.npy',\n",
    "\t'subset': 'fund+sentiment',\n",
    "\t'num_layers': 1,\n",
    "\t'hidden_dim': [2**6],\n",
    "\t'dropout': 0.,\n",
    "\t'learning_rate': 0.001,\n",
    "\t'epochs': 512,\n",
    "\t'weighted_loss': False,\n",
    "\t'reg_l1': 0.0,\n",
    "\t'reg_l2': 0.001,\n",
    "\t'batch_size': 300000,\n",
    "\t'criteria': 'Factor_sharpe',\n",
    "\t'ensemble_members': 8,\n",
    "\t'random_seed': 15,\n",
    "\t'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "\t'num_workers': 4\n",
    "}\n",
    "\n",
    "config['split_lists'] = np.load(config['split_lists_path'], allow_pickle = True)\n",
    "config['input_dim'] = len(subset2col[config['subset']])"
   ],
   "id": "fbf2dd22cac95c33",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T10:21:12.426379Z",
     "start_time": "2024-10-11T10:21:12.342350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = DeepNetwork(config).to(device=config['device'])\n",
    "print(model)"
   ],
   "id": "8e5ece52bc6fe185",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNetwork(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=14, out_features=64, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T10:21:27.419063Z",
     "start_time": "2024-10-11T10:21:13.479846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "losses = {}\n",
    "sharpes = {}\n",
    "for subset in ['flow+fund_mom+sentiment', 'stock+sentiment', 'stock+fund+sentiment']:\n",
    "\tprint('\\nTRAINING FOR SUBSET: {}'.format(subset))\n",
    "\tconfig['subset'] = subset\n",
    "\tconfig['input_dim'] = len(subset2col[config['subset']])\n",
    "\tlosses[subset], sharpes[subset] = run_one_subset(config)"
   ],
   "id": "e5b259d9db71fe01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold no. 1\n",
      "\n",
      "TRAINING ENSEMBLE MEMBER 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubset\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m subset\n\u001B[0;32m      5\u001B[0m config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_dim\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(subset2col[config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubset\u001B[39m\u001B[38;5;124m'\u001B[39m]])\n\u001B[1;32m----> 6\u001B[0m losses[subset], sharpes[subset] \u001B[38;5;241m=\u001B[39m \u001B[43mrun_one_subset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\salab\\PycharmProjects\\ML-mutual-fund\\train.py:180\u001B[0m, in \u001B[0;36mrun_one_subset\u001B[1;34m(config)\u001B[0m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fold \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(crossval_loaders)):\n\u001B[0;32m    179\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRunning fold no. \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(fold \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m--> 180\u001B[0m     avg_loss_ens, avg_sharpe_ens \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_ensembles\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcrossval_loaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmasks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    181\u001B[0m \u001B[43m                                                   \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mensemble_members\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    183\u001B[0m     avg_loss_fold[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m avg_loss_ens[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    184\u001B[0m     avg_loss_fold[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m avg_loss_ens[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32mD:\\Users\\salab\\PycharmProjects\\ML-mutual-fund\\train.py:152\u001B[0m, in \u001B[0;36mtrain_ensembles\u001B[1;34m(config, crossval_loaders, masks, fold, ensemble_members)\u001B[0m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTRAINING ENSEMBLE MEMBER \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(run \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m    151\u001B[0m model \u001B[38;5;241m=\u001B[39m DeepNetwork(config)\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m--> 152\u001B[0m model, loss_train, loss_val, sharpe_train, sharpe_val, best_epoch \u001B[38;5;241m=\u001B[39m \u001B[43mtraining\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    153\u001B[0m \u001B[43m                                                                             \u001B[49m\u001B[43mtrain_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m                                                                             \u001B[49m\u001B[43mrun\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    155\u001B[0m test_loss, test_sharpe \u001B[38;5;241m=\u001B[39m validate(model, test_loader, test_mask, torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mMSELoss(), config)\n\u001B[0;32m    157\u001B[0m avg_loss_ens[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss_train[best_epoch]\n",
      "File \u001B[1;32mD:\\Users\\salab\\PycharmProjects\\ML-mutual-fund\\train.py:73\u001B[0m, in \u001B[0;36mtraining\u001B[1;34m(model, train_loader, val_loader, train_mask, val_mask, config, fold, run)\u001B[0m\n\u001B[0;32m     71\u001B[0m time_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m'\u001B[39m]):\n\u001B[1;32m---> 73\u001B[0m     epoch_loss, epoch_sharpe \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m     loss_train\u001B[38;5;241m.\u001B[39mappend(epoch_loss)\n\u001B[0;32m     80\u001B[0m     sharpe_train\u001B[38;5;241m.\u001B[39mappend(epoch_sharpe)\n",
      "File \u001B[1;32mD:\\Users\\salab\\PycharmProjects\\ML-mutual-fund\\train.py:14\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[1;34m(model, train_loader, mask, optimizer, criterion, config)\u001B[0m\n\u001B[0;32m     11\u001B[0m y_train \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     12\u001B[0m y_pred_train \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 14\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdevice\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdevice\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\salab\\PycharmProjects\\ML-mutual-fund\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\Users\\salab\\PycharmProjects\\ML-mutual-fund\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1327\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1324\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[0;32m   1326\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1327\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1328\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1329\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[0;32m   1330\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Users\\salab\\PycharmProjects\\ML-mutual-fund\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1293\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1289\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[0;32m   1290\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[0;32m   1291\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1292\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m-> 1293\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1294\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[0;32m   1295\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32mD:\\Users\\salab\\PycharmProjects\\ML-mutual-fund\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1131\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m_utils\u001B[38;5;241m.\u001B[39mMP_STATUS_CHECK_INTERVAL):\n\u001B[0;32m   1119\u001B[0m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[0;32m   1120\u001B[0m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1128\u001B[0m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[0;32m   1129\u001B[0m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[0;32m   1130\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1131\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1132\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[0;32m   1133\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1134\u001B[0m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[0;32m   1136\u001B[0m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\queues.py:113\u001B[0m, in \u001B[0;36mQueue.get\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[0;32m    112\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[1;32m--> 113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    114\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll():\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py:256\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[1;32m--> 256\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py:329\u001B[0m, in \u001B[0;36mPipeConnection._poll\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_got_empty_message \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m    327\u001B[0m             _winapi\u001B[38;5;241m.\u001B[39mPeekNamedPipe(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m    328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 329\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py:1066\u001B[0m, in \u001B[0;36mwait\u001B[1;34m(object_list, timeout)\u001B[0m\n\u001B[0;32m   1063\u001B[0m                 ready_objects\u001B[38;5;241m.\u001B[39madd(o)\n\u001B[0;32m   1064\u001B[0m                 timeout \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1066\u001B[0m     ready_handles \u001B[38;5;241m=\u001B[39m \u001B[43m_exhaustive_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwaithandle_to_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1067\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1068\u001B[0m     \u001B[38;5;66;03m# request that overlapped reads stop\u001B[39;00m\n\u001B[0;32m   1069\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ov \u001B[38;5;129;01min\u001B[39;00m ov_list:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\connection.py:998\u001B[0m, in \u001B[0;36m_exhaustive_wait\u001B[1;34m(handles, timeout)\u001B[0m\n\u001B[0;32m    996\u001B[0m ready \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    997\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m L:\n\u001B[1;32m--> 998\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43m_winapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mWaitForMultipleObjects\u001B[49m\u001B[43m(\u001B[49m\u001B[43mL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    999\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;241m==\u001B[39m WAIT_TIMEOUT:\n\u001B[0;32m   1000\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING FOLD NO. 1\n",
      "\n",
      "TRAINING ENSEMBLE MEMBER 1\n",
      "Epoch 1 - Training Loss: 0.00719792, Val Loss: 0.00672514, Train Sharpe: -0.17059104, Validation Sharpe: -0.16196084\n",
      "Best model updated at epoch 1\n",
      "Epoch 2 - Training Loss: 0.00589450, Val Loss: 0.00551381, Train Sharpe: -0.16686632, Validation Sharpe: -0.20798695\n",
      "Epoch 3 - Training Loss: 0.00486706, Val Loss: 0.00459242, Train Sharpe: -0.16529635, Validation Sharpe: -0.20762973\n",
      "Epoch 4 - Training Loss: 0.00410437, Val Loss: 0.00393244, Train Sharpe: -0.17694706, Validation Sharpe: -0.20383722\n",
      "Epoch 5 - Training Loss: 0.00357805, Val Loss: 0.00348807, Train Sharpe: -0.17902026, Validation Sharpe: -0.17700179\n",
      "Epoch 6 - Training Loss: 0.00324291, Val Loss: 0.00319936, Train Sharpe: -0.17015506, Validation Sharpe: -0.17527135\n",
      "Epoch 7 - Training Loss: 0.00304036, Val Loss: 0.00300372, Train Sharpe: -0.17288339, Validation Sharpe: -0.09014613\n",
      "Best model updated at epoch 7\n",
      "Epoch 8 - Training Loss: 0.00291008, Val Loss: 0.00284890, Train Sharpe: -0.14180298, Validation Sharpe: -0.05809058\n",
      "Best model updated at epoch 8\n",
      "Epoch 9 - Training Loss: 0.00280283, Val Loss: 0.00269982, Train Sharpe: -0.12476510, Validation Sharpe: -0.07181480\n",
      "Epoch 10 - Training Loss: 0.00268694, Val Loss: 0.00253916, Train Sharpe: -0.13200901, Validation Sharpe: -0.07042725\n",
      "Epoch 11 - Training Loss: 0.00254833, Val Loss: 0.00236369, Train Sharpe: -0.16662660, Validation Sharpe: -0.03763281\n",
      "Best model updated at epoch 11\n",
      "Epoch 12 - Training Loss: 0.00238658, Val Loss: 0.00217927, Train Sharpe: -0.15525422, Validation Sharpe: -0.08807168\n",
      "Epoch 13 - Training Loss: 0.00220974, Val Loss: 0.00199625, Train Sharpe: -0.14638509, Validation Sharpe: -0.09383417\n",
      "Epoch 14 - Training Loss: 0.00202972, Val Loss: 0.00182577, Train Sharpe: -0.14687946, Validation Sharpe: -0.06373993\n",
      "Epoch 15 - Training Loss: 0.00185886, Val Loss: 0.00167723, Train Sharpe: -0.15818055, Validation Sharpe: -0.05463330\n",
      "Epoch 16 - Training Loss: 0.00170747, Val Loss: 0.00155673, Train Sharpe: -0.16532031, Validation Sharpe: -0.04141683\n",
      "Epoch 17 - Training Loss: 0.00158232, Val Loss: 0.00146613, Train Sharpe: -0.15453675, Validation Sharpe: -0.02015770\n",
      "Best model updated at epoch 17\n",
      "Epoch 18 - Training Loss: 0.00148595, Val Loss: 0.00140308, Train Sharpe: -0.15466166, Validation Sharpe: -0.00484756\n",
      "Best model updated at epoch 18\n",
      "Epoch 19 - Training Loss: 0.00141657, Val Loss: 0.00136177, Train Sharpe: -0.13211560, Validation Sharpe: 0.01456793\n",
      "Best model updated at epoch 19\n",
      "Epoch 20 - Training Loss: 0.00136895, Val Loss: 0.00133433, Train Sharpe: -0.09358706, Validation Sharpe: -0.00030014\n",
      "Epoch 21 - Training Loss: 0.00133569, Val Loss: 0.00131251, Train Sharpe: -0.06602056, Validation Sharpe: 0.01297919\n",
      "Epoch 22 - Training Loss: 0.00130893, Val Loss: 0.00128938, Train Sharpe: -0.07266695, Validation Sharpe: 0.01513345\n",
      "Best model updated at epoch 22\n",
      "Epoch 23 - Training Loss: 0.00128198, Val Loss: 0.00126038, Train Sharpe: -0.06442122, Validation Sharpe: 0.01668152\n",
      "Best model updated at epoch 23\n",
      "Epoch 24 - Training Loss: 0.00125036, Val Loss: 0.00122380, Train Sharpe: -0.07926562, Validation Sharpe: 0.01570804\n",
      "Epoch 25 - Training Loss: 0.00121232, Val Loss: 0.00118053, Train Sharpe: -0.06561971, Validation Sharpe: 0.02122877\n",
      "Best model updated at epoch 25\n",
      "Epoch 26 - Training Loss: 0.00116853, Val Loss: 0.00113327, Train Sharpe: -0.06861995, Validation Sharpe: 0.04911324\n",
      "Best model updated at epoch 26\n",
      "Epoch 27 - Training Loss: 0.00112147, Val Loss: 0.00108563, Train Sharpe: -0.07621479, Validation Sharpe: 0.05431971\n",
      "Best model updated at epoch 27\n",
      "Epoch 28 - Training Loss: 0.00107445, Val Loss: 0.00104110, Train Sharpe: -0.06505221, Validation Sharpe: 0.03127520\n",
      "Epoch 29 - Training Loss: 0.00103074, Val Loss: 0.00100237, Train Sharpe: -0.06476380, Validation Sharpe: 0.04218506\n",
      "Epoch 30 - Training Loss: 0.00099286, Val Loss: 0.00097081, Train Sharpe: -0.08584511, Validation Sharpe: 0.03755736\n",
      "Epoch 31 - Training Loss: 0.00096207, Val Loss: 0.00094638, Train Sharpe: -0.08780488, Validation Sharpe: 0.02835567\n",
      "Epoch 32 - Training Loss: 0.00093829, Val Loss: 0.00092783, Train Sharpe: -0.08244765, Validation Sharpe: 0.01345024\n",
      "Epoch 33 - Training Loss: 0.00092031, Val Loss: 0.00091316, Train Sharpe: -0.08579573, Validation Sharpe: 0.02167948\n",
      "Epoch 34 - Training Loss: 0.00090617, Val Loss: 0.00090015, Train Sharpe: -0.07906737, Validation Sharpe: 0.03854564\n",
      "Epoch 35 - Training Loss: 0.00089369, Val Loss: 0.00088692, Train Sharpe: -0.08099619, Validation Sharpe: 0.02762928\n",
      "Epoch 36 - Training Loss: 0.00088103, Val Loss: 0.00087229, Train Sharpe: -0.07996818, Validation Sharpe: 0.03254993\n",
      "Epoch 37 - Training Loss: 0.00086698, Val Loss: 0.00085593, Train Sharpe: -0.07821880, Validation Sharpe: 0.02564811\n",
      "Epoch 38 - Training Loss: 0.00085117, Val Loss: 0.00083825, Train Sharpe: -0.07343599, Validation Sharpe: 0.02119163\n",
      "Epoch 39 - Training Loss: 0.00083397, Val Loss: 0.00082020, Train Sharpe: -0.06092293, Validation Sharpe: 0.02397760\n",
      "Epoch 40 - Training Loss: 0.00081626, Val Loss: 0.00080290, Train Sharpe: -0.06438636, Validation Sharpe: 0.03606438\n",
      "Epoch 41 - Training Loss: 0.00079910, Val Loss: 0.00078731, Train Sharpe: -0.05076467, Validation Sharpe: 0.03436353\n",
      "Epoch 42 - Training Loss: 0.00078344, Val Loss: 0.00077402, Train Sharpe: -0.05794661, Validation Sharpe: 0.04579934\n",
      "Epoch 43 - Training Loss: 0.00076990, Val Loss: 0.00076310, Train Sharpe: -0.05787956, Validation Sharpe: 0.04743365\n",
      "Epoch 44 - Training Loss: 0.00075862, Val Loss: 0.00075418, Train Sharpe: -0.06036306, Validation Sharpe: 0.03546957\n",
      "Epoch 45 - Training Loss: 0.00074931, Val Loss: 0.00074656, Train Sharpe: -0.05133499, Validation Sharpe: 0.03708049\n",
      "Epoch 46 - Training Loss: 0.00074136, Val Loss: 0.00073943, Train Sharpe: -0.06646351, Validation Sharpe: 0.03718525\n",
      "Epoch 47 - Training Loss: 0.00073405, Val Loss: 0.00073208, Train Sharpe: -0.06477312, Validation Sharpe: 0.03681647\n",
      "Epoch 48 - Training Loss: 0.00072673, Val Loss: 0.00072404, Train Sharpe: -0.06864561, Validation Sharpe: 0.03899320\n",
      "Epoch 49 - Training Loss: 0.00071896, Val Loss: 0.00071518, Train Sharpe: -0.06604600, Validation Sharpe: 0.04267502\n",
      "Epoch 50 - Training Loss: 0.00071058, Val Loss: 0.00070564, Train Sharpe: -0.07175047, Validation Sharpe: 0.03371821\n",
      "Epoch 51 - Training Loss: 0.00070169, Val Loss: 0.00069580, Train Sharpe: -0.06742791, Validation Sharpe: 0.02787142\n",
      "Epoch 101 - Training Loss: 0.00049890, Val Loss: 0.00049454, Train Sharpe: 0.04251504, Validation Sharpe: 0.03432587\n",
      "Best model updated at epoch 126\n",
      "Epoch 126 - Training Loss: 0.00045891, Val Loss: 0.00045529, Train Sharpe: 0.00435737, Validation Sharpe: 0.06181837\n",
      "Best model updated at epoch 129\n",
      "Epoch 129 - Training Loss: 0.00045529, Val Loss: 0.00045175, Train Sharpe: 0.01652859, Validation Sharpe: 0.06265213\n",
      "Best model updated at epoch 130\n",
      "Epoch 130 - Training Loss: 0.00045413, Val Loss: 0.00045062, Train Sharpe: 0.00962037, Validation Sharpe: 0.06292892\n",
      "Best model updated at epoch 138\n",
      "Epoch 138 - Training Loss: 0.00044555, Val Loss: 0.00044225, Train Sharpe: 0.01346728, Validation Sharpe: 0.07133219\n",
      "Best model updated at epoch 139\n",
      "Epoch 139 - Training Loss: 0.00044456, Val Loss: 0.00044128, Train Sharpe: 0.00703288, Validation Sharpe: 0.07448344\n",
      "Best model updated at epoch 144\n",
      "Epoch 144 - Training Loss: 0.00043986, Val Loss: 0.00043669, Train Sharpe: -0.00092012, Validation Sharpe: 0.07812397\n",
      "Best model updated at epoch 145\n",
      "Epoch 145 - Training Loss: 0.00043897, Val Loss: 0.00043582, Train Sharpe: 0.00174606, Validation Sharpe: 0.08384833\n",
      "Best model updated at epoch 147\n",
      "Epoch 147 - Training Loss: 0.00043723, Val Loss: 0.00043412, Train Sharpe: 0.00478498, Validation Sharpe: 0.08938769\n",
      "Best model updated at epoch 148\n",
      "Epoch 148 - Training Loss: 0.00043638, Val Loss: 0.00043330, Train Sharpe: 0.00168672, Validation Sharpe: 0.10114224\n",
      "Epoch 151 - Training Loss: 0.00043393, Val Loss: 0.00043089, Train Sharpe: 0.01590023, Validation Sharpe: 0.09579027\n",
      "Best model updated at epoch 159\n",
      "Epoch 159 - Training Loss: 0.00042797, Val Loss: 0.00042499, Train Sharpe: 0.02839086, Validation Sharpe: 0.11272829\n",
      "Best model updated at epoch 160\n",
      "Epoch 160 - Training Loss: 0.00042727, Val Loss: 0.00042430, Train Sharpe: 0.03045329, Validation Sharpe: 0.12794004\n",
      "Best model updated at epoch 163\n",
      "Epoch 163 - Training Loss: 0.00042526, Val Loss: 0.00042230, Train Sharpe: 0.01273115, Validation Sharpe: 0.12824719\n",
      "Best model updated at epoch 165\n",
      "Epoch 165 - Training Loss: 0.00042397, Val Loss: 0.00042102, Train Sharpe: 0.01973137, Validation Sharpe: 0.13076498\n",
      "Best model updated at epoch 169\n",
      "Epoch 169 - Training Loss: 0.00042151, Val Loss: 0.00041857, Train Sharpe: 0.03464811, Validation Sharpe: 0.13423067\n",
      "Epoch 201 - Training Loss: 0.00040650, Val Loss: 0.00040353, Train Sharpe: 0.04206542, Validation Sharpe: 0.07783172\n",
      "Epoch 251 - Training Loss: 0.00039366, Val Loss: 0.00039081, Train Sharpe: 0.05080371, Validation Sharpe: 0.08076215\n",
      "Epoch 301 - Training Loss: 0.00038764, Val Loss: 0.00038525, Train Sharpe: 0.18521626, Validation Sharpe: 0.10789943\n",
      "Best model updated at epoch 323\n",
      "Epoch 323 - Training Loss: 0.00038612, Val Loss: 0.00038403, Train Sharpe: 0.22345936, Validation Sharpe: 0.13719982\n",
      "Best model updated at epoch 324\n",
      "Epoch 324 - Training Loss: 0.00038607, Val Loss: 0.00038398, Train Sharpe: 0.21837720, Validation Sharpe: 0.13749698\n",
      "Best model updated at epoch 325\n",
      "Epoch 325 - Training Loss: 0.00038601, Val Loss: 0.00038393, Train Sharpe: 0.21119639, Validation Sharpe: 0.14736231\n",
      "Best model updated at epoch 326\n",
      "Epoch 326 - Training Loss: 0.00038595, Val Loss: 0.00038389, Train Sharpe: 0.22077931, Validation Sharpe: 0.15357387\n",
      "Epoch 351 - Training Loss: 0.00038483, Val Loss: 0.00038289, Train Sharpe: 0.23448332, Validation Sharpe: 0.10531011\n",
      "Best model updated at epoch 400\n",
      "Epoch 400 - Training Loss: 0.00038362, Val Loss: 0.00038142, Train Sharpe: 0.32057926, Validation Sharpe: 0.15663207\n",
      "Epoch 401 - Training Loss: 0.00038360, Val Loss: 0.00038140, Train Sharpe: 0.32848725, Validation Sharpe: 0.15810888\n",
      "Best model updated at epoch 401\n",
      "Best model updated at epoch 402\n",
      "Epoch 402 - Training Loss: 0.00038359, Val Loss: 0.00038137, Train Sharpe: 0.33009541, Validation Sharpe: 0.16258891\n",
      "Best model updated at epoch 403\n",
      "Epoch 403 - Training Loss: 0.00038357, Val Loss: 0.00038135, Train Sharpe: 0.33027151, Validation Sharpe: 0.16419987\n",
      "Best model updated at epoch 406\n",
      "Epoch 406 - Training Loss: 0.00038353, Val Loss: 0.00038128, Train Sharpe: 0.32462022, Validation Sharpe: 0.16545844\n",
      "Best model updated at epoch 407\n",
      "Epoch 407 - Training Loss: 0.00038351, Val Loss: 0.00038126, Train Sharpe: 0.31607044, Validation Sharpe: 0.17273881\n",
      "Best model updated at epoch 418\n",
      "Epoch 418 - Training Loss: 0.00038337, Val Loss: 0.00038103, Train Sharpe: 0.30499300, Validation Sharpe: 0.17800404\n",
      "Best model updated at epoch 419\n",
      "Epoch 419 - Training Loss: 0.00038336, Val Loss: 0.00038101, Train Sharpe: 0.30431849, Validation Sharpe: 0.17898487\n",
      "Best model updated at epoch 421\n",
      "Best model updated at epoch 422\n",
      "Epoch 422 - Training Loss: 0.00038333, Val Loss: 0.00038096, Train Sharpe: 0.31236926, Validation Sharpe: 0.19098912\n",
      "Epoch 451 - Training Loss: 0.00038310, Val Loss: 0.00038058, Train Sharpe: 0.32813746, Validation Sharpe: 0.14325373\n",
      "Best model updated at epoch 453\n",
      "Epoch 453 - Training Loss: 0.00038308, Val Loss: 0.00038056, Train Sharpe: 0.33298206, Validation Sharpe: 0.25725955\n",
      "Best model updated at epoch 454\n",
      "Epoch 454 - Training Loss: 0.00038308, Val Loss: 0.00038056, Train Sharpe: 0.33035892, Validation Sharpe: 0.26677892\n",
      "Best model updated at epoch 458\n",
      "Epoch 458 - Training Loss: 0.00038306, Val Loss: 0.00038052, Train Sharpe: 0.33502334, Validation Sharpe: 0.27079543\n",
      "Best model updated at epoch 460\n",
      "Epoch 460 - Training Loss: 0.00038304, Val Loss: 0.00038051, Train Sharpe: 0.32736760, Validation Sharpe: 0.27198872\n",
      "Best model updated at epoch 461\n",
      "Best model updated at epoch 464\n",
      "Epoch 464 - Training Loss: 0.00038302, Val Loss: 0.00038048, Train Sharpe: 0.33088350, Validation Sharpe: 0.27429727\n",
      "Best model updated at epoch 466\n",
      "Epoch 466 - Training Loss: 0.00038301, Val Loss: 0.00038046, Train Sharpe: 0.32138804, Validation Sharpe: 0.27896163\n",
      "Best model updated at epoch 468\n",
      "Epoch 468 - Training Loss: 0.00038301, Val Loss: 0.00038045, Train Sharpe: 0.32795402, Validation Sharpe: 0.27920485\n",
      "Best model updated at epoch 470\n",
      "Epoch 470 - Training Loss: 0.00038300, Val Loss: 0.00038043, Train Sharpe: 0.32578245, Validation Sharpe: 0.29352459\n",
      "Best model updated at epoch 471\n",
      "Best model updated at epoch 472\n",
      "Epoch 472 - Training Loss: 0.00038299, Val Loss: 0.00038042, Train Sharpe: 0.32422432, Validation Sharpe: 0.30167961\n",
      "Best model updated at epoch 474\n",
      "Epoch 474 - Training Loss: 0.00038298, Val Loss: 0.00038041, Train Sharpe: 0.32218131, Validation Sharpe: 0.30448711\n",
      "Best model updated at epoch 486\n",
      "Epoch 486 - Training Loss: 0.00038294, Val Loss: 0.00038035, Train Sharpe: 0.32524621, Validation Sharpe: 0.30548686\n",
      "Epoch 501 - Training Loss: 0.00038290, Val Loss: 0.00038028, Train Sharpe: 0.33304432, Validation Sharpe: 0.29391882\n",
      "Best model updated at epoch 509\n",
      "Epoch 509 - Training Loss: 0.00038288, Val Loss: 0.00038026, Train Sharpe: 0.34049472, Validation Sharpe: 0.30969796\n",
      "Best model updated at epoch 510\n",
      "Epoch 510 - Training Loss: 0.00038288, Val Loss: 0.00038025, Train Sharpe: 0.34003323, Validation Sharpe: 0.31111965\n",
      "Training completed in 59m 44s\n",
      "\n",
      "TRAINING ENSEMBLE MEMBER 2\n",
      "Epoch 1 - Training Loss: 0.01085727, Val Loss: 0.00904050, Train Sharpe: 0.04186894, Validation Sharpe: 0.23062798\n",
      "Best model updated at epoch 1\n",
      "Epoch 2 - Training Loss: 0.00891137, Val Loss: 0.00727123, Train Sharpe: 0.05428691, Validation Sharpe: 0.25335291\n",
      "Best model updated at epoch 2\n",
      "Epoch 3 - Training Loss: 0.00724341, Val Loss: 0.00580881, Train Sharpe: 0.06566380, Validation Sharpe: 0.26165932\n",
      "Best model updated at epoch 3\n",
      "Epoch 4 - Training Loss: 0.00585306, Val Loss: 0.00464693, Train Sharpe: 0.06360865, Validation Sharpe: 0.26029199\n",
      "Epoch 5 - Training Loss: 0.00473497, Val Loss: 0.00377184, Train Sharpe: 0.06423649, Validation Sharpe: 0.26921746\n",
      "Best model updated at epoch 5\n",
      "Epoch 6 - Training Loss: 0.00387740, Val Loss: 0.00316086, Train Sharpe: 0.05938958, Validation Sharpe: 0.27179128\n",
      "Best model updated at epoch 6\n",
      "Epoch 7 - Training Loss: 0.00326070, Val Loss: 0.00278115, Train Sharpe: 0.06443069, Validation Sharpe: 0.29609311\n",
      "Best model updated at epoch 7\n",
      "Epoch 8 - Training Loss: 0.00285626, Val Loss: 0.00258986, Train Sharpe: 0.06660326, Validation Sharpe: 0.26901403\n",
      "Epoch 9 - Training Loss: 0.00262673, Val Loss: 0.00253652, Train Sharpe: 0.07115576, Validation Sharpe: 0.27821556\n",
      "Epoch 10 - Training Loss: 0.00252806, Val Loss: 0.00256803, Train Sharpe: 0.10243592, Validation Sharpe: 0.29230013\n",
      "Epoch 11 - Training Loss: 0.00251357, Val Loss: 0.00263465, Train Sharpe: 0.10315828, Validation Sharpe: 0.29317605\n",
      "Epoch 12 - Training Loss: 0.00253918, Val Loss: 0.00269562, Train Sharpe: 0.09892782, Validation Sharpe: 0.30425557\n",
      "Best model updated at epoch 12\n",
      "Epoch 13 - Training Loss: 0.00256824, Val Loss: 0.00272256, Train Sharpe: 0.09846520, Validation Sharpe: 0.30207542\n",
      "Epoch 14 - Training Loss: 0.00257458, Val Loss: 0.00270001, Train Sharpe: 0.09457125, Validation Sharpe: 0.29898015\n",
      "Epoch 15 - Training Loss: 0.00254328, Val Loss: 0.00262402, Train Sharpe: 0.10309223, Validation Sharpe: 0.31912315\n",
      "Best model updated at epoch 15\n",
      "Epoch 16 - Training Loss: 0.00246951, Val Loss: 0.00249952, Train Sharpe: 0.09956048, Validation Sharpe: 0.33367521\n",
      "Best model updated at epoch 16\n",
      "Epoch 17 - Training Loss: 0.00235640, Val Loss: 0.00233730, Train Sharpe: 0.11348326, Validation Sharpe: 0.32006696\n",
      "Epoch 18 - Training Loss: 0.00221243, Val Loss: 0.00215122, Train Sharpe: 0.12446150, Validation Sharpe: 0.31447262\n",
      "Epoch 19 - Training Loss: 0.00204900, Val Loss: 0.00195593, Train Sharpe: 0.12890951, Validation Sharpe: 0.32382089\n",
      "Epoch 20 - Training Loss: 0.00187832, Val Loss: 0.00176504, Train Sharpe: 0.10471083, Validation Sharpe: 0.33210528\n",
      "Epoch 21 - Training Loss: 0.00171187, Val Loss: 0.00158986, Train Sharpe: 0.10493688, Validation Sharpe: 0.32082167\n",
      "Epoch 22 - Training Loss: 0.00155920, Val Loss: 0.00143858, Train Sharpe: 0.11270773, Validation Sharpe: 0.31606379\n",
      "Epoch 23 - Training Loss: 0.00142722, Val Loss: 0.00131593, Train Sharpe: 0.11920416, Validation Sharpe: 0.31560382\n",
      "Epoch 24 - Training Loss: 0.00131984, Val Loss: 0.00122317, Train Sharpe: 0.11602481, Validation Sharpe: 0.31380495\n",
      "Epoch 25 - Training Loss: 0.00123801, Val Loss: 0.00115845, Train Sharpe: 0.12318585, Validation Sharpe: 0.31486005\n",
      "Epoch 26 - Training Loss: 0.00118001, Val Loss: 0.00111749, Train Sharpe: 0.13415965, Validation Sharpe: 0.31171408\n",
      "Epoch 27 - Training Loss: 0.00114204, Val Loss: 0.00109437, Train Sharpe: 0.14326030, Validation Sharpe: 0.32585448\n",
      "Epoch 28 - Training Loss: 0.00111891, Val Loss: 0.00108252, Train Sharpe: 0.13661781, Validation Sharpe: 0.30371815\n",
      "Epoch 29 - Training Loss: 0.00110490, Val Loss: 0.00107556, Train Sharpe: 0.12249722, Validation Sharpe: 0.30449122\n",
      "Epoch 30 - Training Loss: 0.00109452, Val Loss: 0.00106805, Train Sharpe: 0.12328345, Validation Sharpe: 0.30059829\n",
      "Epoch 31 - Training Loss: 0.00108315, Val Loss: 0.00105606, Train Sharpe: 0.11968190, Validation Sharpe: 0.29833254\n",
      "Epoch 32 - Training Loss: 0.00106747, Val Loss: 0.00103735, Train Sharpe: 0.11878993, Validation Sharpe: 0.29716113\n",
      "Epoch 33 - Training Loss: 0.00104567, Val Loss: 0.00101136, Train Sharpe: 0.11737830, Validation Sharpe: 0.30102134\n",
      "Epoch 34 - Training Loss: 0.00101741, Val Loss: 0.00097896, Train Sharpe: 0.11832672, Validation Sharpe: 0.30252254\n",
      "Epoch 35 - Training Loss: 0.00098361, Val Loss: 0.00094206, Train Sharpe: 0.12596592, Validation Sharpe: 0.29878631\n",
      "Epoch 36 - Training Loss: 0.00094603, Val Loss: 0.00090314, Train Sharpe: 0.09460995, Validation Sharpe: 0.30162495\n",
      "Epoch 37 - Training Loss: 0.00090696, Val Loss: 0.00086480, Train Sharpe: 0.08728926, Validation Sharpe: 0.29665127\n",
      "Epoch 38 - Training Loss: 0.00086873, Val Loss: 0.00082932, Train Sharpe: 0.08217559, Validation Sharpe: 0.29542229\n",
      "Epoch 39 - Training Loss: 0.00083336, Val Loss: 0.00079841, Train Sharpe: 0.07936934, Validation Sharpe: 0.30553037\n",
      "Epoch 40 - Training Loss: 0.00080238, Val Loss: 0.00077304, Train Sharpe: 0.07550942, Validation Sharpe: 0.31675637\n",
      "Epoch 41 - Training Loss: 0.00077662, Val Loss: 0.00075340, Train Sharpe: 0.08083551, Validation Sharpe: 0.29710716\n",
      "Epoch 42 - Training Loss: 0.00075622, Val Loss: 0.00073897, Train Sharpe: 0.08545052, Validation Sharpe: 0.28091797\n",
      "Epoch 43 - Training Loss: 0.00074069, Val Loss: 0.00072873, Train Sharpe: 0.07732352, Validation Sharpe: 0.28357127\n",
      "Epoch 44 - Training Loss: 0.00072910, Val Loss: 0.00072134, Train Sharpe: 0.06433855, Validation Sharpe: 0.27903801\n",
      "Epoch 45 - Training Loss: 0.00072023, Val Loss: 0.00071542, Train Sharpe: 0.05752978, Validation Sharpe: 0.29486376\n",
      "Epoch 46 - Training Loss: 0.00071285, Val Loss: 0.00070972, Train Sharpe: 0.06060864, Validation Sharpe: 0.29782790\n",
      "Epoch 47 - Training Loss: 0.00070584, Val Loss: 0.00070332, Train Sharpe: 0.06002637, Validation Sharpe: 0.28696314\n",
      "Epoch 48 - Training Loss: 0.00069836, Val Loss: 0.00069566, Train Sharpe: 0.05599356, Validation Sharpe: 0.28199533\n",
      "Epoch 49 - Training Loss: 0.00068993, Val Loss: 0.00068660, Train Sharpe: 0.05601984, Validation Sharpe: 0.28156790\n",
      "Epoch 50 - Training Loss: 0.00068041, Val Loss: 0.00067636, Train Sharpe: 0.04695319, Validation Sharpe: 0.27270630\n",
      "Epoch 51 - Training Loss: 0.00066998, Val Loss: 0.00066538, Train Sharpe: 0.04901887, Validation Sharpe: 0.27405971\n",
      "Epoch 101 - Training Loss: 0.00049242, Val Loss: 0.00049671, Train Sharpe: 0.03361644, Validation Sharpe: 0.14234672\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "crossval_loaders, masks = get_crossval_dataloaders(config['data_path'], config['split_lists'], config['subset'], batch_size=config['batch_size'])\n",
    "\n",
    "avg_loss_fold = {'train': 0.0, 'val': 0.0, 'test': 0.0}\n",
    "avg_sharpe_fold = {'train': 0.0, 'val': 0.0, 'test': 0.0}\n",
    "\n",
    "for fold in range(len(crossval_loaders)):\n",
    "\tprint('\\nRUNNING FOLD NO. {}'.format(fold+1))\n",
    "\tavg_loss_ens, avg_sharpe_ens = train_ensembles(config, crossval_loaders, masks, fold, config['ensemble_members'])\n",
    "\t\n",
    "\tavg_loss_fold['train'] += avg_loss_ens['train']\n",
    "\tavg_loss_fold['val'] += avg_loss_ens['val']\n",
    "\tavg_loss_fold['test'] += avg_loss_ens['test']\n",
    "\t\n",
    "\tavg_sharpe_fold['train'] += avg_sharpe_ens['train']\n",
    "\tavg_sharpe_fold['val'] += avg_sharpe_ens['val']\n",
    "\tavg_sharpe_fold['test'] += avg_sharpe_ens['test']\n",
    "\n",
    "avg_loss_fold = {key: value / (len(crossval_loaders)) for key, value in avg_loss_fold.items()}\n",
    "avg_sharpe_fold = {key: value / (len(crossval_loaders)) for key, value in avg_sharpe_fold.items()}\n",
    "\n",
    "print(avg_loss_fold)\n",
    "print(avg_sharpe_fold)"
   ],
   "id": "7aca6348ec83e019"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.32411616676669497\n",
      "0.16614131398356732\n",
      "-0.14898776745329467\n",
      "-0.10232087341214076\n"
     ]
    }
   ],
   "execution_count": 16,
   "source": [
    "avg_sharpe_fold_ens = 0.\n",
    "for fold in range(len(crossval_loaders)):\n",
    "\tmodel_dirs = os.path.join('./Experiments/', config['subset'], 'fold' + str(fold+1))\n",
    "\tensemble = DeepEnsemble(config, model_dirs)\n",
    "\t\n",
    "\tcrossval_loaders, masks = get_crossval_dataloaders(config['data_path'], config['split_lists'], config['subset'], batch_size=config['batch_size'])\n",
    "\t_, _, test_loader = crossval_loaders[fold]['dataloaders']\n",
    "\tX_test, y_test = unload_data(test_loader)\n",
    "\t_, _, test_mask = masks[fold]\n",
    "\t\n",
    "\tens_pred_test = ensemble.predict(X_test.float().to(config['device']))\n",
    "\tsharpe_test = evaluate_sharpe(ens_pred_test, y_test, test_mask)\n",
    "\tprint(sharpe_test)\n",
    "\tavg_sharpe_fold_ens += sharpe_test\n",
    "\n",
    "avg_sharpe_fold_ens /= len(crossval_loaders)\n",
    "print(avg_sharpe_fold_ens)"
   ],
   "id": "a9facfea55596a11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tlosses, sharpes = run_all_subsets(config)\n",
    "\tprint(losses)\n",
    "\tprint(sharpes)"
   ],
   "id": "978880446dddc07b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
